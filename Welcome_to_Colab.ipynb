{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangitaGIT011/Myproj/blob/main/Welcome_to_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# Step 1: Upload your CSV file\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(next(iter(uploaded)))\n",
        "\n",
        "# Step 2: Inspect dataset\n",
        "print(\"Columns:\", df.columns)\n",
        "print(df.head())\n",
        "\n",
        "# Step 3: Ensure numeric types\n",
        "df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce')\n",
        "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
        "\n",
        "# Step 4: Aggregate sold quantity by product\n",
        "agg = df.groupby('Product', as_index=False).agg({'Quantity':'sum', 'Price':'mean'})\n",
        "\n",
        "# --- Pie Chart for Quantity Sold ---\n",
        "plt.figure(figsize=(7,7))\n",
        "\n",
        "plt.pie(\n",
        "    agg['Quantity'],\n",
        "    labels=agg['Product'],\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=140\n",
        ")\n",
        "plt.title('Quantity Sold by Product')\n",
        "plt.show()\n",
        "\n",
        "# --- Histogram for Price Distribution ---\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.hist(df['Price'].dropna(), bins=10, color='skyblue', edgecolor='black')\n",
        "plt.title('Price Distribution')\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JwfuiEdOZuop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Suppose your file is data.csv\n",
        "df = pd.read_csv(\"data.csv\")\n"
      ],
      "metadata": {
        "id": "gSlkLam-wmx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import files\n",
        "\n",
        "# Step 1: Upload and load dataset\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(\"retail_sales_dataset.csv\")\n",
        "\n",
        "print(\"Original Dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 2: Introduce artificial missing values\n",
        "np.random.seed(42)\n",
        "mask = np.random.choice([True, False], size=df.shape, p=[0.05, 0.95])\n",
        "df_missing = df.mask(mask)\n",
        "\n",
        "print(\"\\nDataset with Artificial Missing Values:\")\n",
        "print(df_missing.head(10))\n",
        "\n",
        "# Step 3: Handle missing values using mean imputation\n",
        "df_imputed = df_missing.fillna(df_missing.mean(numeric_only=True))\n",
        "\n",
        "print(\"\\nDataset after Mean Imputation:\")\n",
        "print(df_imputed.head(10))\n",
        "\n",
        "# Step 4: Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
        "scaled_features = scaler.fit_transform(df_imputed[numeric_cols])\n",
        "\n",
        "df_scaled = df_imputed.copy()\n",
        "df_scaled[numeric_cols] = scaled_features\n",
        "\n",
        "print(\"\\nNormalized Dataset:\")\n",
        "print(df_scaled.head())\n",
        "\n",
        "# Step 5: Split into training and testing sets\n",
        "# If you have a target column, specify it here. Otherwise, we just split features.\n",
        "if 'target' in df_scaled.columns:\n",
        "    X = df_scaled.drop(columns=['target'])\n",
        "    y = df_scaled['target']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    print(\"\\nTraining Set Shape:\", X_train.shape)\n",
        "    print(\"Testing Set Shape:\", X_test.shape)\n",
        "else:\n",
        "    # No target column ‚Üí just split features\n",
        "    X_train, X_test = train_test_split(df_scaled, test_size=0.2, random_state=42)\n",
        "    print(\"\\nTraining Set Shape:\", X_train.shape)\n",
        "    print(\"Testing Set Shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "G-lNuFDwx07g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 2: Load a dataset\n",
        "# For demo, we'll use the Iris dataset from sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "#df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "#df['target'] = iris.target\n",
        "\n",
        "#print(\"Original Dataset:\")\n",
        "#print(df.head())\n",
        "\n",
        "# Step 3: Introduce artificial missing values\n",
        "np.random.seed(42)  # reproducibility\n",
        "# Randomly select 5% of the data to be replaced with NaN\n",
        "mask = np.random.choice([True, False], size=df.shape, p=[0.05, 0.95])\n",
        "df_missing = df.mask(mask)\n",
        "\n",
        "print(\"\\nDataset with Artificial Missing Values:\")\n",
        "print(df_missing.head(10))\n",
        "\n",
        "# Step 4: Handle missing values using mean imputation\n",
        "df_imputed = df_missing.fillna(df_missing.mean(numeric_only=True))\n",
        "\n",
        "print(\"\\nDataset after Mean Imputation:\")\n",
        "print(df_imputed.head(10))\n",
        "\n",
        "# Step 5: Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "features = df_imputed.drop(columns=['target'])\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "df_scaled = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "df_scaled['target'] = df_imputed['target']\n",
        "\n",
        "print(\"\\nNormalized Dataset:\")\n",
        "print(df_scaled.head())\n",
        "\n",
        "# Step 6: Split into training and testing sets\n",
        "X = df_scaled.drop(columns=['target'])\n",
        "y = df_scaled['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Set Shape:\", X_train.shape)\n",
        "print(\"Testing Set Shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "2wQzU5EIys7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 2: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Convert to DataFrame for easier manipulation\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "print(\"‚úÖ Original dataset shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Step 3: Introduce artificial missing values\n",
        "np.random.seed(42)  # reproducibility\n",
        "missing_indices = [(np.random.randint(0, df.shape[0]), np.random.randint(0, df.shape[1]-1)) for _ in range(10)]\n",
        "for row, col in missing_indices:\n",
        "    df.iat[row, col] = np.nan\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è Dataset with artificial missing values:\")\n",
        "print(df.head(15))\n",
        "\n",
        "# Step 4: Handle missing values using mean imputation\n",
        "df.fillna(df.mean(numeric_only=True), inplace=True)\n",
        "\n",
        "print(\"\\n‚úÖ Dataset after mean imputation:\")\n",
        "print(df.head(15))\n",
        "\n",
        "# Step 5: Normalize numerical features (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df.iloc[:, :-1])  # exclude target column\n",
        "\n",
        "# Step 6: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nüìä Training set shape:\", X_train.shape)\n",
        "print(\"üìä Testing set shape:\", X_test.shape)\n",
        "\n",
        "# Step 7: Verify normalization (mean ~0, std ~1)\n",
        "print(\"\\nüîé Feature means after scaling:\", X_train.mean(axis=0))\n",
        "print(\"üîé Feature std devs after scaling:\", X_train.std(axis=0))\n"
      ],
      "metadata": {
        "id": "Dp6twFNM44dN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}