{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangitaGIT011/Myproj/blob/main/Welcome_to_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# Step 1: Upload your CSV file\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(next(iter(uploaded)))\n",
        "\n",
        "# Step 2: Inspect dataset\n",
        "print(\"Columns:\", df.columns)\n",
        "print(df.head())\n",
        "\n",
        "# Step 3: Ensure numeric types\n",
        "df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce')\n",
        "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
        "\n",
        "# Step 4: Aggregate sold quantity by product\n",
        "agg = df.groupby('Product', as_index=False).agg({'Quantity':'sum', 'Price':'mean'})\n",
        "\n",
        "# --- Pie Chart for Quantity Sold ---\n",
        "plt.figure(figsize=(7,7))\n",
        "\n",
        "plt.pie(\n",
        "    agg['Quantity'],\n",
        "    labels=agg['Product'],\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=140\n",
        ")\n",
        "plt.title('Quantity Sold by Product')\n",
        "plt.show()\n",
        "\n",
        "# --- Histogram for Price Distribution ---\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.hist(df['Price'].dropna(), bins=10, color='skyblue', edgecolor='black')\n",
        "plt.title('Price Distribution')\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JwfuiEdOZuop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Suppose your file is data.csv\n",
        "df = pd.read_csv(\"data.csv\")\n"
      ],
      "metadata": {
        "id": "gSlkLam-wmx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import files\n",
        "\n",
        "# Step 1: Upload and load dataset\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(\"retail_sales_dataset.csv\")\n",
        "\n",
        "print(\"Original Dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 2: Introduce artificial missing values\n",
        "np.random.seed(42)\n",
        "mask = np.random.choice([True, False], size=df.shape, p=[0.05, 0.95])\n",
        "df_missing = df.mask(mask)\n",
        "\n",
        "print(\"\\nDataset with Artificial Missing Values:\")\n",
        "print(df_missing.head(10))\n",
        "\n",
        "# Step 3: Handle missing values using mean imputation\n",
        "df_imputed = df_missing.fillna(df_missing.mean(numeric_only=True))\n",
        "\n",
        "print(\"\\nDataset after Mean Imputation:\")\n",
        "print(df_imputed.head(10))\n",
        "\n",
        "# Step 4: Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
        "scaled_features = scaler.fit_transform(df_imputed[numeric_cols])\n",
        "\n",
        "df_scaled = df_imputed.copy()\n",
        "df_scaled[numeric_cols] = scaled_features\n",
        "\n",
        "print(\"\\nNormalized Dataset:\")\n",
        "print(df_scaled.head())\n",
        "\n",
        "# Step 5: Split into training and testing sets\n",
        "# If you have a target column, specify it here. Otherwise, we just split features.\n",
        "if 'target' in df_scaled.columns:\n",
        "    X = df_scaled.drop(columns=['target'])\n",
        "    y = df_scaled['target']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    print(\"\\nTraining Set Shape:\", X_train.shape)\n",
        "    print(\"Testing Set Shape:\", X_test.shape)\n",
        "else:\n",
        "    # No target column â†’ just split features\n",
        "    X_train, X_test = train_test_split(df_scaled, test_size=0.2, random_state=42)\n",
        "    print(\"\\nTraining Set Shape:\", X_train.shape)\n",
        "    print(\"Testing Set Shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "G-lNuFDwx07g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 2: Load a dataset\n",
        "# For demo, we'll use the Iris dataset from sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "print(\"Original Dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 3: Introduce artificial missing values\n",
        "np.random.seed(42)  # reproducibility\n",
        "# Randomly select 5% of the data to be replaced with NaN\n",
        "mask = np.random.choice([True, False], size=df.shape, p=[0.05, 0.95])\n",
        "df_missing = df.mask(mask)\n",
        "\n",
        "print(\"\\nDataset with Artificial Missing Values:\")\n",
        "print(df_missing.head(10))\n",
        "\n",
        "# Step 4: Handle missing values using mean imputation\n",
        "df_imputed = df_missing.fillna(df_missing.mean(numeric_only=True))\n",
        "\n",
        "print(\"\\nDataset after Mean Imputation:\")\n",
        "print(df_imputed.head(10))\n",
        "\n",
        "# Step 5: Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "features = df_imputed.drop(columns=['target'])\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "df_scaled = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "df_scaled['target'] = df_imputed['target']\n",
        "\n",
        "print(\"\\nNormalized Dataset:\")\n",
        "print(df_scaled.head())\n",
        "\n",
        "# Step 6: Split into training and testing sets\n",
        "X = df_scaled.drop(columns=['target'])\n",
        "y = df_scaled['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Set Shape:\", X_train.shape)\n",
        "print(\"Testing Set Shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "2wQzU5EIys7k",
        "outputId": "bb297f81-168b-42cb-a8e5-a70d5e11254f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "\n",
            "Dataset with Artificial Missing Values:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                NaN               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "5                5.4               3.9                1.7               0.4   \n",
            "6                4.6               3.4                1.4               0.3   \n",
            "7                5.0               3.4                1.5               0.2   \n",
            "8                4.4               2.9                NaN               0.2   \n",
            "9                4.9               3.1                1.5               0.1   \n",
            "\n",
            "   target  \n",
            "0     0.0  \n",
            "1     0.0  \n",
            "2     0.0  \n",
            "3     0.0  \n",
            "4     0.0  \n",
            "5     NaN  \n",
            "6     0.0  \n",
            "7     0.0  \n",
            "8     0.0  \n",
            "9     0.0  \n",
            "\n",
            "Dataset after Mean Imputation:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0           5.100000               3.5           1.400000               0.2   \n",
            "1           4.900000               3.0           1.400000               0.2   \n",
            "2           5.855396               3.2           1.300000               0.2   \n",
            "3           4.600000               3.1           1.500000               0.2   \n",
            "4           5.000000               3.6           1.400000               0.2   \n",
            "5           5.400000               3.9           1.700000               0.4   \n",
            "6           4.600000               3.4           1.400000               0.3   \n",
            "7           5.000000               3.4           1.500000               0.2   \n",
            "8           4.400000               2.9           3.783571               0.2   \n",
            "9           4.900000               3.1           1.500000               0.1   \n",
            "\n",
            "   target  \n",
            "0     0.0  \n",
            "1     0.0  \n",
            "2     0.0  \n",
            "3     0.0  \n",
            "4     0.0  \n",
            "5     1.0  \n",
            "6     0.0  \n",
            "7     0.0  \n",
            "8     0.0  \n",
            "9     0.0  \n",
            "\n",
            "Normalized Dataset:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0      -9.643256e-01          1.009780          -1.387060         -1.393791   \n",
            "1      -1.219642e+00         -0.144254          -1.387060         -1.393791   \n",
            "2      -1.133834e-15          0.317359          -1.445253         -1.393791   \n",
            "3      -1.602617e+00          0.086553          -1.328868         -1.393791   \n",
            "4      -1.091984e+00          1.240587          -1.387060         -1.393791   \n",
            "\n",
            "   target  \n",
            "0     0.0  \n",
            "1     0.0  \n",
            "2     0.0  \n",
            "3     0.0  \n",
            "4     0.0  \n",
            "\n",
            "Training Set Shape: (120, 4)\n",
            "Testing Set Shape: (30, 4)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}