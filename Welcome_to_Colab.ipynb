{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangitaGIT011/Myproj/blob/main/Welcome_to_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# Step 1: Upload your CSV file\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(next(iter(uploaded)))\n",
        "\n",
        "# Step 2: Inspect dataset\n",
        "print(\"Columns:\", df.columns)\n",
        "print(df.head())\n",
        "\n",
        "# Step 3: Ensure numeric types\n",
        "df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce')\n",
        "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
        "\n",
        "# Step 4: Aggregate sold quantity by product\n",
        "agg = df.groupby('Product', as_index=False).agg({'Quantity':'sum', 'Price':'mean'})\n",
        "\n",
        "# --- Pie Chart for Quantity Sold ---\n",
        "plt.figure(figsize=(7,7))\n",
        "\n",
        "plt.pie(\n",
        "    agg['Quantity'],\n",
        "    labels=agg['Product'],\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=140\n",
        ")\n",
        "plt.title('Quantity Sold by Product')\n",
        "plt.show()\n",
        "\n",
        "# --- Histogram for Price Distribution ---\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.hist(df['Price'].dropna(), bins=10, color='skyblue', edgecolor='black')\n",
        "plt.title('Price Distribution')\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JwfuiEdOZuop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Suppose your file is data.csv\n",
        "df = pd.read_csv(\"data.csv\")\n"
      ],
      "metadata": {
        "id": "gSlkLam-wmx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import files\n",
        "\n",
        "# Step 1: Upload and load dataset\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(\"retail_sales_dataset.csv\")\n",
        "\n",
        "print(\"Original Dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 2: Introduce artificial missing values\n",
        "np.random.seed(42)\n",
        "mask = np.random.choice([True, False], size=df.shape, p=[0.05, 0.95])\n",
        "df_missing = df.mask(mask)\n",
        "\n",
        "print(\"\\nDataset with Artificial Missing Values:\")\n",
        "print(df_missing.head(10))\n",
        "\n",
        "# Step 3: Handle missing values using mean imputation\n",
        "df_imputed = df_missing.fillna(df_missing.mean(numeric_only=True))\n",
        "\n",
        "print(\"\\nDataset after Mean Imputation:\")\n",
        "print(df_imputed.head(10))\n",
        "\n",
        "# Step 4: Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
        "scaled_features = scaler.fit_transform(df_imputed[numeric_cols])\n",
        "\n",
        "df_scaled = df_imputed.copy()\n",
        "df_scaled[numeric_cols] = scaled_features\n",
        "\n",
        "print(\"\\nNormalized Dataset:\")\n",
        "print(df_scaled.head())\n",
        "\n",
        "# Step 5: Split into training and testing sets\n",
        "# If you have a target column, specify it here. Otherwise, we just split features.\n",
        "if 'target' in df_scaled.columns:\n",
        "    X = df_scaled.drop(columns=['target'])\n",
        "    y = df_scaled['target']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    print(\"\\nTraining Set Shape:\", X_train.shape)\n",
        "    print(\"Testing Set Shape:\", X_test.shape)\n",
        "else:\n",
        "    # No target column â†’ just split features\n",
        "    X_train, X_test = train_test_split(df_scaled, test_size=0.2, random_state=42)\n",
        "    print(\"\\nTraining Set Shape:\", X_train.shape)\n",
        "    print(\"Testing Set Shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "G-lNuFDwx07g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}